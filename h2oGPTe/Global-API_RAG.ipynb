{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "977c3b5b",
   "metadata": {},
   "source": [
    "## Global API - RAG\n",
    "Global API（任意のCollectionに対してではなくh2oGPTe本体に対するAPI）を用いたh2oGPTeに対する操作とRAGの実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c01b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2ogpte import H2OGPTE\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95d4378d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection-specific-API.ipynb     _secret\r\n",
      "Collection-specific-API_RAG.ipynb \u001b[34mdata\u001b[m\u001b[m\r\n",
      "Global-API_RAG.ipynb              readme.md\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1c37bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('_secret') as f:\n",
    "    key = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d48766a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Server version 1.5.8 doesn't match client version 1.5.5: unexpected errors may occur.\n",
      "Please install the correct version of H2OGPTE with `pip install h2ogpte==1.5.8`.\n",
      "You can enable strict version checking by passing strict_version_check=True.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<h2ogpte.h2ogpte.H2OGPTE at 0x105ebc880>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = H2OGPTE(\n",
    "    address='https://h2ogpte.internal.dedicated.h2o.ai',\n",
    "    api_key=key['global_internal-dedicated-0726'],\n",
    ")\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eed1e15",
   "metadata": {},
   "source": [
    "### Collectionの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a00b2645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BAAI/bge-large-en-v1.5', 'BAAI/bge-m3', 'hkunlp/instructor-large', 'off']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_embedding_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2dbea2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'17c3c8c8-f7cb-4f6d-9c50-c369d4f41518'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new collection\n",
    "collection_id = client.create_collection(\n",
    "    name='resume-analyzer',\n",
    "    description='職務経歴書に対するRAG',\n",
    "    embedding_model='BAAI/bge-m3',\n",
    "    prompt_template_id='16095e26-6876-410f-a934-3c0651a21e4c'    # h2oGPTe UI（チャットの設定）から取得\n",
    ")\n",
    "collection_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6829cf9d",
   "metadata": {},
   "source": [
    "create_collection(): https://h2oai.github.io/h2ogpte/h2ogpte.html#h2ogpte.h2ogpte.H2OGPTE.create_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb001ff",
   "metadata": {},
   "source": [
    "### Documentのアップロードと知識データベース化（chunking, indexing etc）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af9c533b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resume_TaroShonan.docx\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2ddb749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'112e2257-b266-4bc8-8a7e-1d49e3619e89'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload documents\n",
    "with open('data/resume_TaroShonan.docx', 'rb') as f:\n",
    "    upload_file = client.upload('resume_TaroShonan.docx', f)\n",
    "\n",
    "upload_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d029720",
   "metadata": {},
   "source": [
    "upload(): https://h2oai.github.io/h2ogpte/h2ogpte.html#h2ogpte.h2ogpte.H2OGPTE.upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9aa528d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Job(id='e152ac53-264f-47aa-81bf-307b124c2ef7', name='Adding documents', passed=1.0, failed=0.0, progress=1.0, completed=True, canceled=False, date=datetime.datetime(2024, 7, 26, 2, 27, 38, tzinfo=TzInfo(UTC)), kind=<JobKind.IngestUploadsJob: 'IngestUploadsJob'>, statuses=[JobStatus(id='c4184dc7ee4847c0b02f20562af230aa', status='Indexing done.'), JobStatus(id='8e32806b56614437a80c4004ffc916b5', status='Collecting done.')], errors=[], last_update_date=datetime.datetime(2024, 7, 26, 2, 27, 45, tzinfo=TzInfo(UTC)), duration='7s', duration_seconds=7.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ingest documents (Creates previews, chunks and embeddings)\n",
    "client.ingest_uploads(collection_id, [upload_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcf57ca",
   "metadata": {},
   "source": [
    "ingest_uploads(): https://h2oai.github.io/h2ogpte/h2ogpte.html#h2ogpte.h2ogpte.H2OGPTE.ingest_uploads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63d853f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocumentInfo(id='66191b88-dfe5-466b-9ea3-9768c5098739', username='yuki.shimada@h2o.ai', name='resume_TaroShonan.pdf', type='PDF', size=115555, page_count=3, pii_settings=None, connector='Upload', uri=None, original_type='Word', meta_data_dict={}, status='completed', updated_at=datetime.datetime(2024, 7, 26, 2, 27, 45, 271093, tzinfo=TzInfo(UTC)))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collection内のDocument\n",
    "client.list_documents_in_collection(collection_id, offset=0, limit=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049d351",
   "metadata": {},
   "source": [
    "### チャット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fe26bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mistralai/Mixtral-8x7B-Instruct-v0.1',\n",
       " 'mistralai/Mistral-7B-Instruct-v0.3',\n",
       " 'meta-llama/Meta-Llama-3.1-8B-Instruct',\n",
       " 'meta-llama/Meta-Llama-3.1-70B-Instruct',\n",
       " 'OpenGVLab/InternVL-Chat-V1-5',\n",
       " 'h2oai/h2o-danube3-4b-chat',\n",
       " 'mistral-medium-latest',\n",
       " 'claude-3-5-sonnet-20240620',\n",
       " 'claude-3-haiku-20240307',\n",
       " 'gpt-35-turbo-1106',\n",
       " 'gpt-4-1106-preview',\n",
       " 'gpt-4o',\n",
       " 'gemini-1.5-pro-latest']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_llm_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e0c313e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9d5362e4-8124-4985-99ab-1b3113448419'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a chat session\n",
    "chat_session_id = client.create_chat_session(collection_id)\n",
    "chat_session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3020b176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "この職務経歴書は「湘南 太郎」という人物のものです。この人は、5年以上の経験を持ったデータ活用領域のデータサイエンティストやコンサルタントであり、顧客の課題理解、分析やモデル作成、プロジェクトマネジメント、顧客マネジメントまで幅広く対応できます。また、元々大学でマーケティングを勉強し、その後マーケティングのコンサルティング会社で勤務していたため、関連業務に明るいとのことです。\n",
      "\n",
      "この人の得意な業務は、以下の通りです。\n",
      "\n",
      "* 顧客へのヒアリングから要件定義、プロジェクトマネジメントまでの経験\n",
      "* データ分析と顧客への結果報告\n",
      "* 5名規模のチームマネジメント経験\n",
      "* マーケティング関連業務の知識\n",
      "\n",
      "さらに、この人はPython、AWS、画像処理、統計解析、データビジュアライゼーションなどの技術スキルを持っています。\n",
      "CPU times: user 9.3 ms, sys: 4.11 ms, total: 13.4 ms\n",
      "Wall time: 8.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with client.connect(chat_session_id) as session:\n",
    "    reply = session.query(\n",
    "        '誰の職務経歴書になりますか？また、この人の得意な業務を教えて下さい。',\n",
    "        llm='mistralai/Mixtral-8x7B-Instruct-v0.1',\n",
    "        rag_config={\"rag_type\": \"rag\"},   # (Normal) RAG \n",
    "    )\n",
    "    print(reply.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6cb6f004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taro Shonanの技術スキルは、Python, AWS, 画像処理, 統計解析, データビジュアライゼーション, R, SQL, 自然言語処理, 生成AI, Linux, Docker, Githubです。\n",
      "CPU times: user 6.09 ms, sys: 2.9 ms, total: 8.99 ms\n",
      "Wall time: 3.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with client.connect(chat_session_id) as session:\n",
    "    reply = session.query(\n",
    "        '書かれている技術スキルをあげて下さい。',\n",
    "        llm='mistralai/Mixtral-8x7B-Instruct-v0.1',\n",
    "        rag_config={\"rag_type\": \"rag\"},   # (Normal) RAG \n",
    "    )\n",
    "    print(reply.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a9fb1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4ecc42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"count\": 14}\n",
      "CPU times: user 5.07 ms, sys: 2.26 ms, total: 7.33 ms\n",
      "Wall time: 2.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with client.connect(chat_session_id) as session:\n",
    "    reply = session.query(\n",
    "        '書かれている技術スキルをあげて下さい。',\n",
    "        llm='mistralai/Mixtral-8x7B-Instruct-v0.1',\n",
    "        rag_config={\"rag_type\": \"rag\"},   # (Normal) RAG \n",
    "        llm_args=dict(\n",
    "            response_format='json_object',\n",
    "            guided_json={\n",
    "                '$schema': 'http://json-schema.org/draft-07/schema#',\n",
    "                'type': 'object',\n",
    "                'properties': {'count': {'type': 'integer'}},\n",
    "                'required': [\n",
    "                    'count',\n",
    "                ],\n",
    "            },\n",
    "        ),\n",
    "    )\n",
    "    print(reply.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3489d35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(id='971520be-a904-4e8a-bba5-3879df41c2ca', content='{\"count\": 12}', reply_to='f1050c4a-aeaa-4a8e-a67f-6f5fd742f379', votes=0, created_at=datetime.datetime(2024, 7, 26, 11, 48, 2, 790085), type_list=[], error=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "777b99bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"skills\": [\"Python\", \"AWS\", \"\\u753b\\u50cf\\u51e6\\u7406\", \"\\u7d71\\u8a08\\u89e3\\u6790\", \"\\u30c7\\u30fc\\u30bf\\u30d3\\u30b8\\u30e5\\u30a2\\u30e9\\u30a4\\u30bc\\u30fc\\u30b7\\u30e7\\u30f3\", \"R\", \"SQL\", \"\\u81ea\\u7136\\u8a00\\u8a9e\\u51e6\\u7406\", \"\\u751f\\u6210AI\", \"Linux\", \"Docker\", \"Github\"]}\n",
      "CPU times: user 6.24 ms, sys: 2.34 ms, total: 8.58 ms\n",
      "Wall time: 3.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with client.connect(chat_session_id) as session:\n",
    "    reply = session.query(\n",
    "        '書かれている技術スキルをあげて下さい。',\n",
    "        llm='mistralai/Mixtral-8x7B-Instruct-v0.1',\n",
    "        rag_config={\"rag_type\": \"rag\"},   # (Normal) RAG \n",
    "        llm_args=dict(\n",
    "            response_format='json_object',\n",
    "        ),\n",
    "    )\n",
    "    print(reply.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d7e4a709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'skills': ['Python',\n",
       "  'AWS',\n",
       "  '画像処理',\n",
       "  '統計解析',\n",
       "  'データビジュアライゼーション',\n",
       "  'R',\n",
       "  'SQL',\n",
       "  '自然言語処理',\n",
       "  '生成AI',\n",
       "  'Linux',\n",
       "  'Docker',\n",
       "  'Github']}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(reply.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396abf9b",
   "metadata": {},
   "source": [
    "query(): https://h2oai.github.io/h2ogpte/h2ogpte.html#h2ogpte.session.Session.query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d921011b",
   "metadata": {},
   "source": [
    "llm_args\n",
    "- temperature (float, default: 0) — The value used to modulate the next token probabilities. Most deterministic: 0, Most creative: 1 \n",
    "- top_k (int, default: 1) — The number of highest probability vocabulary tokens to keep for top-k-filtering. \n",
    "- top_p (float, default: 1.0) — If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation. \n",
    "- seed (int, default: 0) — The seed for the random number generator when sampling during generation (if temp>0 or top_k>1 or top_p<1), seed=0 picks a random seed. \n",
    "- repetition_penalty (float, default: 1.07) — The parameter for repetition penalty. 1.0 means no penalty. \n",
    "- max_new_tokens (int, default: 1024) — Maximum number of new tokens to generate. This limit applies to each (map+reduce) step during summarization and each (map) step during extraction. \n",
    "- min_max_new_tokens (int, default: 512) — minimum value for max_new_tokens when auto-adjusting for content of prompt, docs, etc. \n",
    "- response_format (str, default: “text”) — Output type, one of [“text”, “json_object”, “json_code”]. \n",
    "- guided_json (str, default: “”) — If specified, the output will follow the JSON schema. \n",
    "- guided_regex (str, default: “”) — If specified, the output will follow the regex pattern. \n",
    "- guided_choice (Optional[List[str]], default: None — If specified, the output will be exactly one of the choices. \n",
    "- guided_grammar (str, default: “”) — If specified, the output will follow the context free grammar. \n",
    "- guided_whitespace_pattern (str, default: “”) — If specified, will override the default whitespace pattern for guided json decoding. \n",
    "- enable_vision (str, default: “auto”) - Controls vision mode, send images to the LLM in addition to text chunks. Only if have models that support vision, use get_vision_capable_llm_names() to see list. One of [“on”, “off”, “auto”]. \n",
    "- visible_vision_models (List[str], default: [“auto”]) - Controls which vision model to use when processing images. Use get_vision_capable_llm_names() to see list. Must provide exactly one model. [“auto”] for automatic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "26057241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"management_skills\": [\"\\u30c1\\u30fc\\u30e0\\u7ba1\\u7406\", \"\\u30d7\\u30ed\\u30b8\\u30a7\\u30af\\u30c8\\u7ba1\\u7406\", \"\\u6226\\u7565\\u7acb\\u6848\", \"\\u9867\\u5ba2\\u5bfe\\u5fdc\", \"\\u30c1\\u30fc\\u30e0\\u30e1\\u30f3\\u30d0\\u30fc\\u306e\\u63a1\\u7528\\u3001\\u30c8\\u30ec\\u30fc\\u30cb\\u30f3\\u30b0\\u3001\\u304a\\u3088\\u3073\\u6307\\u5c0e\", \"\\u30c1\\u30fc\\u30e0\\u30e1\\u30f3\\u30d0\\u30fc\\u306e\\u696d\\u7e3e\\u8a55\\u4fa1\\u3068\\u30d5\\u30a3\\u30fc\\u30c9\\u30d0\\u30c3\\u30af\\u306e\\u63d0\\u4f9b\", \"\\u30c1\\u30fc\\u30e0\\u306e\\u30b9\\u30ad\\u30eb\\u958b\\u767a\\u3068\\u30ad\\u30e3\\u30ea\\u30a2\\u6210\\u9577\\u306e\\u30b5\\u30dd\\u30fc\\u30c8\", \"\\u30c7\\u30fc\\u30bf\\u53ce\\u96c6\\u3001\\u4fdd\\u5b58\\u3001\\u7ba1\\u7406\\u306e\\u305f\\u3081\\u306e\\u30dd\\u30ea\\u30b7\\u30fc\\u3068\\u30d7\\u30ed\\u30bb\\u30b9\\u306e\\u7b56\\u5b9a\", \"\\u30c7\\u30fc\\u30bf\\u306e\\u54c1\\u8cea\\u7ba1\\u7406\\u3068\\u30ac\\u30d0\\u30ca\\u30f3\\u30b9\\u306e\\u78ba\\u4fdd\", \"\\u30c7\\u30fc\\u30bf\\u6226\\u7565\\u306e\\u7b56\\u5b9a\", \"\\u30c7\\u30fc\\u30bf\\u30a4\\u30f3\\u30d5\\u30e9\\u306e\\u8a2d\\u8a08\\u3068\\u5b9f\\u88c5\", \"\\u30af\\u30e9\\u30a4\\u30a2\\u30f3\\u30c8\\u306e\\u5f93\\u696d\\u54e1\\u306b\\u5bfe\\u3059\\u308b\\u30c7\\u30fc\\u30bf\\u30ea\\u30c6\\u30e9\\u30b7\\u30fc\\u3084\\u5206\\u6790\\u30b9\\u30ad\\u30eb\\u306e\\u30c8\\u30ec\\u30fc\\u30cb\\u30f3\\u30b0\", \"\\u30c7\\u30fc\\u30bf\\u30d7\\u30ed\\u30b8\\u30a7\\u30af\\u30c8\\u306e\\u8a08\\u753b\\u3001\\u5b9f\\u884c\\u3001\\u9032\\u6357\\u7ba1\\u7406\", \"\\u30d7\\u30ed\\u30b8\\u30a7\\u30af\\u30c8\\u306e\\u6210\\u679c\\u7269\\u306e\\u54c1\\u8cea\\u4fdd\\u8a3c\\u3068\\u7d0d\\u54c1\", \"\\u30b3\\u30f3\\u30b5\\u30eb\\u30c6\\u30a3\\u30f3\\u30b0\", \"\\u55b6\\u696d\"]}\n",
      "CPU times: user 11.8 ms, sys: 4.38 ms, total: 16.2 ms\n",
      "Wall time: 7.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with client.connect(chat_session_id) as session:\n",
    "    reply = session.query(\n",
    "        '書かれているマネジメント関連のスキルをあげて下さい。',\n",
    "        llm='mistralai/Mixtral-8x7B-Instruct-v0.1',\n",
    "        rag_config={\"rag_type\": \"rag\"},   # (Normal) RAG \n",
    "        llm_args=dict(\n",
    "            response_format='json_object',\n",
    "        ),\n",
    "    )\n",
    "    print(reply.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "56c54eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'management_skills': ['チーム管理',\n",
       "  'プロジェクト管理',\n",
       "  '戦略立案',\n",
       "  '顧客対応',\n",
       "  'チームメンバーの採用、トレーニング、および指導',\n",
       "  'チームメンバーの業績評価とフィードバックの提供',\n",
       "  'チームのスキル開発とキャリア成長のサポート',\n",
       "  'データ収集、保存、管理のためのポリシーとプロセスの策定',\n",
       "  'データの品質管理とガバナンスの確保',\n",
       "  'データ戦略の策定',\n",
       "  'データインフラの設計と実装',\n",
       "  'クライアントの従業員に対するデータリテラシーや分析スキルのトレーニング',\n",
       "  'データプロジェクトの計画、実行、進捗管理',\n",
       "  'プロジェクトの成果物の品質保証と納品',\n",
       "  'コンサルティング',\n",
       "  '営業']}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(reply.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c2680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llm] *",
   "language": "python",
   "name": "conda-env-llm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
