{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "174e53da",
   "metadata": {},
   "source": [
    "## Collection-specific API - Conversation\n",
    "Collection-specific API（事前に作成したCollectionのAPIキーを利用）を利用したLLMの利用。RAGなし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "660c860b-51c2-4ace-86a8-06a881ba0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2ogpte import H2OGPTE\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb211ace-af85-487a-96e7-9557349052e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('_secret') as f:\n",
    "    key = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e19e3d5-be0e-4949-b50a-314c600e7b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h2ogpte.h2ogpte.H2OGPTE at 0x10aa3c2e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = H2OGPTE(\n",
    "    address='https://playground.h2ogpte.h2o.ai',\n",
    "    api_key=key['client-access-test'],\n",
    ")\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1eb986f1-118f-4332-8a94-5b8b527d826a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'mistralai/Mixtral-8x7B-Instruct-v0.1'),\n",
       " (1, 'meta-llama/Meta-Llama-3-8B-Instruct'),\n",
       " (2, 'meta-llama/Meta-Llama-3-70B-Instruct'),\n",
       " (3, 'nvidia/Llama3-ChatQA-1.5-70B'),\n",
       " (4, 'NousResearch/Nous-Capybara-34B'),\n",
       " (5, 'mistralai/Mistral-7B-Instruct-v0.3'),\n",
       " (6, 'h2oai/h2o-danube2-1.8b-chat'),\n",
       " (7, 'mistralai/Mistral-Nemo-Instruct-2407'),\n",
       " (8, 'OpenGVLab/InternVL-Chat-V1-5'),\n",
       " (9, 'THUDM/cogvlm2-llama3-chat-19B'),\n",
       " (10, 'liuhaotian/llava-v1.6-34b'),\n",
       " (11, 'lmms-lab/llama3-llava-next-8b'),\n",
       " (12, 'mistral-small-latest'),\n",
       " (13, 'mistral-large-latest'),\n",
       " (14, 'mistral-medium'),\n",
       " (15, 'claude-3-5-sonnet-20240620'),\n",
       " (16, 'claude-3-sonnet-20240229'),\n",
       " (17, 'claude-3-opus-20240229'),\n",
       " (18, 'claude-3-haiku-20240307'),\n",
       " (19, 'microsoft/Phi-3-vision-128k-instruct'),\n",
       " (20, 'microsoft/Phi-3-medium-128k-instruct'),\n",
       " (21, 'google/gemma-2-27b-it'),\n",
       " (22, 'gemini-1.5-pro-latest'),\n",
       " (23, 'gemini-1.5-flash-latest'),\n",
       " (24, 'gpt-3.5-turbo-0613'),\n",
       " (25, 'gpt-3.5-turbo-16k-0613'),\n",
       " (26, 'gpt-35-turbo-1106'),\n",
       " (27, 'gpt-4-1106-preview'),\n",
       " (28, 'gpt-4-turbo-2024-04-09')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM一覧\n",
    "models = client.get_llms()\n",
    "[(i, m['base_model']) for i,m in enumerate(models)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e2170da-0904-4dc9-be72-9b15fb8bfab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'display_name': 'microsoft/Phi-3-medium-128k-instruct',\n",
       " 'base_model': 'microsoft/Phi-3-medium-128k-instruct',\n",
       " 'prompt_type': 'unknown',\n",
       " 'prompt_dict': {'promptA': None,\n",
       "  'promptB': None,\n",
       "  'PreInstruct': None,\n",
       "  'PreInput': None,\n",
       "  'PreResponse': None,\n",
       "  'terminate_response': [],\n",
       "  'chat_sep': '\\n',\n",
       "  'chat_turn_sep': '\\n',\n",
       "  'humanstr': None,\n",
       "  'botstr': None,\n",
       "  'generates_leading_space': False,\n",
       "  'system_prompt': '',\n",
       "  'can_handle_system_prompt': False},\n",
       " 'chat_template': \"{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\",\n",
       " 'load_8bit': False,\n",
       " 'load_4bit': False,\n",
       " 'low_bit_mode': 1,\n",
       " 'load_half': False,\n",
       " 'use_flash_attention_2': False,\n",
       " 'load_gptq': '',\n",
       " 'load_awq': '',\n",
       " 'load_exllama': False,\n",
       " 'use_safetensors': True,\n",
       " 'revision': None,\n",
       " 'use_gpu_id': False,\n",
       " 'gpu_id': None,\n",
       " 'compile_model': None,\n",
       " 'use_cache': None,\n",
       " 'llamacpp_dict': {'n_gpu_layers': 100,\n",
       "  'use_mlock': True,\n",
       "  'n_batch': 1024,\n",
       "  'n_gqa': 0,\n",
       "  'model_path_llama': '',\n",
       "  'model_name_gptj': '',\n",
       "  'model_name_gpt4all_llama': '',\n",
       "  'model_name_exllama_if_no_config': ''},\n",
       " 'rope_scaling': {},\n",
       " 'max_seq_len': 131002,\n",
       " 'max_output_seq_len': None,\n",
       " 'exllama_dict': {},\n",
       " 'gptq_dict': {},\n",
       " 'attention_sinks': False,\n",
       " 'sink_dict': {},\n",
       " 'truncation_generation': False,\n",
       " 'hf_model_dict': {},\n",
       " 'force_seq2seq_type': False,\n",
       " 'force_t5_type': False,\n",
       " 'trust_remote_code': True,\n",
       " 'llm': True,\n",
       " 'rag': True,\n",
       " 'image': True,\n",
       " 'actually_image': False,\n",
       " 'video': True,\n",
       " 'actually_video': False,\n",
       " 'json': True,\n",
       " 'guided_vllm': True,\n",
       " 'auto_visible_vision_models': 'OpenGVLab/InternVL-Chat-V1-5'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17f7af07-4480-4984-ba98-11ab5fea9d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.get_llm_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1567fff-187f-4433-a1e2-c400d5a47808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c895f7ad-abe1-4677-9645-2b1d04d741ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'73d297dd-c51b-441e-9530-3f6315df9164'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_session_id = client.create_chat_session_on_default_collection()\n",
    "chat_session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aace1858-ec31-4094-a640-d8954948e207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='8f1f7662-7ec5-4549-a54a-063efa08bb25' content='こんにちは！私は h2oGPTe です。H2O.ai によって作成されたエキスパートの質問応答 AI システムです。OpenAI による GPT-4 と同様に機能します。よろしくお願いします。' reply_to='a672fa4a-0a90-4c53-9919-ee92ea7989a6' votes=0 created_at=datetime.datetime(2024, 7, 21, 10, 21, 2, 480253) type_list=[] error=None\n"
     ]
    }
   ],
   "source": [
    "with client.connect(chat_session_id) as session:\n",
    "    reply = session.query(\n",
    "        message='こんにちは。あなたのモデル名は？',\n",
    "        timeout=60,\n",
    "    )\n",
    "    print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "110a9ebd-2922-4745-87dd-8f69e01bacb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'こんにちは！私は h2oGPTe です。H2O.ai によって作成されたエキスパートの質問応答 AI システムです。OpenAI による GPT-4 と同様に機能します。よろしくお願いします。'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02621767-ba34-4f22-933d-a6c29128763e",
   "metadata": {},
   "source": [
    "Session.query(): https://h2oai.github.io/h2ogpte/h2ogpte.html#h2ogpte.session.Session.query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f235cd-2548-4fb3-a0b0-1d907f98ecf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdc54b35-afd5-4f87-8bc7-9119e413c8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは、Phi-3。私のモデル名は「Phi-3」です。\n"
     ]
    }
   ],
   "source": [
    "SYS_PROMPT = '''\n",
    "あなたは日本語AIアシスタントです。必ず日本語で回答します。\n",
    "'''\n",
    "\n",
    "MESSAGE = '''\n",
    "こんにちは、Phi-3。あなたのモデル名をフルネームで答えて？\n",
    "'''\n",
    "\n",
    "MODEL = 'microsoft/Phi-3-medium-128k-instruct'\n",
    "\n",
    "with client.connect(chat_session_id) as session:\n",
    "    reply = session.query(\n",
    "        message = MESSAGE,\n",
    "        system_prompt=SYS_PROMPT,\n",
    "        llm=MODEL,\n",
    "        rag_config={\"rag_type\": \"llm_only\",},\n",
    "    )\n",
    "    print(reply.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3ed7093-21a1-4fbd-abe7-76a626cbc506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "神奈川県には多くの魅力的な観光スポットがあります。以下に、おすすめの5つの観光地をご紹介します。\n",
      "\n",
      "1. 横浜中華街: 日本最大級の中華街で、豊富な中華料理店や雑貨店があります。夜にはライトアップされた街並みが美しいです。\n",
      "\n",
      "2. 鎌倉: 古都として知られる鎌倉は、歴史的な建造物や寺院が多くあります。特に、鎌倉大仏や円覚寺などが有名です。\n",
      "\n",
      "3. 江の島: 横浜市の沖合にある小さな島で、海水浴やショッピングが楽しめます。島の中心にある大仏は、日本でも有名な観光スポットです。\n",
      "\n",
      "4. 小田原城: 小田原城は、江戸時代に建てられた城で、城内には歴史的な建造物や美術館があります。城の周りには緑豊かな公園もあります。\n",
      "\n",
      "5. 箱根: 箱根は、自然豊かな観光地で、箱根ロープウェイや箱根彫刻の森美術館などがあります。また、箱根の温泉も有名です。\n",
      "\n",
      "これらの観光地は、神奈川県の多様な魅力を堪能するのに最適な場所です。\n"
     ]
    }
   ],
   "source": [
    "SYS_PROMPT = '''\n",
    "あなたは日本語AIアシスタントです。必ず日本語で回答します。\n",
    "'''\n",
    "\n",
    "MESSAGE = '''\n",
    "神奈川のおすすめの観光スポットを5つ教えて？\n",
    "'''\n",
    "\n",
    "MODEL = 'microsoft/Phi-3-medium-128k-instruct'\n",
    "\n",
    "with client.connect(chat_session_id) as session:\n",
    "    reply = session.query(\n",
    "        message = MESSAGE,\n",
    "        system_prompt=SYS_PROMPT,\n",
    "        llm=MODEL,\n",
    "        rag_config={\"rag_type\": \"llm_only\",},\n",
    "    )\n",
    "    print(reply.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc98cdf-3cf1-4b98-88e7-0e1137c4ca42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e043c7d-376e-49fe-8e2d-f8f26e1975f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "現在の日本の総理大臣は岸田文雄です。彼の年齢は2023年時点で58歳です。58を3で掛けると174になります。そして、174を2で割ると87になります。\n",
      "\n",
      "したがって、岸田文雄の年齢を3で掛けた値を2で割った結果は87です。\n"
     ]
    }
   ],
   "source": [
    "SYS_PROMPT = '''\n",
    "あなたは日本語AIアシスタントです。必ず日本語で回答します。\n",
    "'''\n",
    "\n",
    "MESSAGE = '''\n",
    "日本の総理大臣は誰ですか？その年齢を掛ける3した値を教えて？\n",
    "'''\n",
    "\n",
    "MODEL = 'microsoft/Phi-3-medium-128k-instruct'\n",
    "\n",
    "with client.connect(chat_session_id) as session:\n",
    "    reply = session.query(\n",
    "        message = MESSAGE,\n",
    "        system_prompt=SYS_PROMPT,\n",
    "        llm=MODEL,\n",
    "        rag_config={\"rag_type\": \"llm_only\",},\n",
    "    )\n",
    "    print(reply.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7551b34-890a-4213-81ab-9d05d430a905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本の総理大臣は岸田文雄です。彼の年齢は2023年2月時点で65歳です。65歳を3倍すると195歳になります。\n"
     ]
    }
   ],
   "source": [
    "SYS_PROMPT = '''\n",
    "あなたは日本語AIアシスタントです。必ず日本語で回答します。\n",
    "'''\n",
    "\n",
    "MESSAGE = '''\n",
    "日本の総理大臣は誰ですか？その年齢を掛ける3した値を教えて？\n",
    "'''\n",
    "\n",
    "MODEL = 'gpt-4-turbo-2024-04-09'\n",
    "\n",
    "with client.connect(chat_session_id) as session:\n",
    "    reply = session.query(\n",
    "        message = MESSAGE,\n",
    "        system_prompt=SYS_PROMPT,\n",
    "        llm=MODEL,\n",
    "        rag_config={\"rag_type\": \"llm_only\",},\n",
    "    )\n",
    "    print(reply.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beafa52-b7d2-47f0-bd19-6cc92dded049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbff1a6f-2d70-48b5-9f0f-81cc593776c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "与えられた決定係数の目安に従って、0.6の決定係数は「0.3~0.7」の範囲に収まるため、中位の精度と判定されます。\n"
     ]
    }
   ],
   "source": [
    "SYS_PROMPT = '''\n",
    "あなたは日本語AIアシスタントです。必ず日本語で回答します。\n",
    "'''\n",
    "\n",
    "r2 = 0.6\n",
    "\n",
    "MESSAGE = '''\n",
    "予測モデルの決定係数が{}でした。以下の決定係数の目安に従って、精度を判定して下さい。\n",
    "\n",
    "### 決定係数の目安\n",
    "0.3以下: 精度が低い\n",
    "0.3~0.7: 中位の精度\n",
    "0.7以上: 精度が高い\n",
    "'''.format(r2)\n",
    "\n",
    "MODEL = 'microsoft/Phi-3-medium-128k-instruct'\n",
    "\n",
    "with client.connect(chat_session_id) as session:\n",
    "    reply = session.query(\n",
    "        message = MESSAGE,\n",
    "        system_prompt=SYS_PROMPT,\n",
    "        llm=MODEL,\n",
    "        rag_config={\"rag_type\": \"llm_only\",},\n",
    "    )\n",
    "    print(reply.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c7970f6-b2a2-49a5-b273-a8559acb4db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "与えられた決定係数の目安に従って、0.1の決定係数は「精度が低い」と判定されます。\n"
     ]
    }
   ],
   "source": [
    "SYS_PROMPT = '''\n",
    "あなたは日本語AIアシスタントです。必ず日本語で回答します。\n",
    "'''\n",
    "\n",
    "r2 = 0.1\n",
    "\n",
    "MESSAGE = '''\n",
    "予測モデルの決定係数が{}でした。以下の決定係数の目安に従って、精度を判定して下さい。\n",
    "\n",
    "### 決定係数の目安\n",
    "0.3以下: 精度が低い\n",
    "0.3~0.7: 中位の精度\n",
    "0.7以上: 精度が高い\n",
    "'''.format(r2)\n",
    "\n",
    "MODEL = 'microsoft/Phi-3-medium-128k-instruct'\n",
    "\n",
    "with client.connect(chat_session_id) as session:\n",
    "    reply = session.query(\n",
    "        message = MESSAGE,\n",
    "        system_prompt=SYS_PROMPT,\n",
    "        llm=MODEL,\n",
    "        rag_config={\"rag_type\": \"llm_only\",},\n",
    "    )\n",
    "    print(reply.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "561c744a-24a9-474a-8d8f-81b776e7ef44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "与えられた決定係数の目安に従って、0.7の決定係数は「0.7以上」の範囲にあります。したがって、この予測モデルは「精度が高い」と判定されます。\n"
     ]
    }
   ],
   "source": [
    "SYS_PROMPT = '''\n",
    "あなたは日本語AIアシスタントです。必ず日本語で回答します。\n",
    "'''\n",
    "\n",
    "r2 = 0.7\n",
    "\n",
    "MESSAGE = '''\n",
    "予測モデルの決定係数が{}でした。以下の決定係数の目安に従って、精度を判定して下さい。\n",
    "\n",
    "### 決定係数の目安\n",
    "0.3以下: 精度が低い\n",
    "0.3~0.7: 中位の精度\n",
    "0.7以上: 精度が高い\n",
    "'''.format(r2)\n",
    "\n",
    "MODEL = 'microsoft/Phi-3-medium-128k-instruct'\n",
    "\n",
    "with client.connect(chat_session_id) as session:\n",
    "    reply = session.query(\n",
    "        message = MESSAGE,\n",
    "        system_prompt=SYS_PROMPT,\n",
    "        llm=MODEL,\n",
    "        rag_config={\"rag_type\": \"llm_only\",},\n",
    "    )\n",
    "    print(reply.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2731a88f-283e-4a77-8b56-377e4a6b1eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "108f9101-853d-49ee-89ee-df4bda509564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "決定係数が0.7であるため、中位の精度と判断できます。変数重要度に['A', 'C', 'G']が現れているため、警報Yを出す必要があります。したがって、精度は中位ですが、警報Yが出ているため注意が必要です。\n",
      "\n",
      "注意喚起:\n",
      "\n",
      "この予測モデルは中位の精度を示していますが、変数重要度に基づいて警報Yが出ています。これは、変数AとCの両方が重要な影響を与えていることを意味します。このモデルを使用する際には、これらの変数の影響をより深く理解し、必要に応じてモデルの改善を検討することが重要です。\n"
     ]
    }
   ],
   "source": [
    "SYS_PROMPT = '''\n",
    "あなたは日本語AIアシスタントです。必ず日本語で回答します。\n",
    "'''\n",
    "\n",
    "r2 = 0.7\n",
    "vars = ['A', 'C', 'G']\n",
    "\n",
    "MESSAGE = '''\n",
    "予測モデルの決定係数が{}でした。以下の決定係数の目安に従って、精度を判定して下さい。\n",
    "また、予測モデルの変数重要度に{}が現れました。以下の注意が必要な変数に従って、警報を出す必要があるか判断して下さい。\n",
    "\n",
    "予測精度も低く、警報も出ている場合は注意が必要ですので、注意喚起を促して下さい。\n",
    "\n",
    "### 決定係数の目安\n",
    "0.3以下: 精度が低い\n",
    "0.3~0.7: 中位の精度\n",
    "0.7以上: 精度が高い\n",
    "\n",
    "### 注意が必要な変数\n",
    "変数AまたはBがあった場合: 警報X\n",
    "変数AとCがあった場合: 警報Y\n",
    "'''.format(r2, vars)\n",
    "\n",
    "MODEL = 'microsoft/Phi-3-medium-128k-instruct'\n",
    "\n",
    "with client.connect(chat_session_id) as session:\n",
    "    reply = session.query(\n",
    "        message = MESSAGE,\n",
    "        system_prompt=SYS_PROMPT,\n",
    "        llm=MODEL,\n",
    "        rag_config={\"rag_type\": \"llm_only\",},\n",
    "    )\n",
    "    print(reply.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba4a276c-6ef9-48fb-8622-75aef1a71ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "決定係数が0.7であるため、予測モデルの精度は「高い」と判定されます。\n",
      "\n",
      "変数重要度に['A', 'C', 'G']が含まれているため、注意が必要な変数の条件に該当します。具体的には、「変数AまたはBがあった場合: 警報X」と「変数AとCがあった場合: 警報Y」の両方の警報が発生します。\n",
      "\n",
      "予測精度は高いですが、警報が発生しているため、モデルの使用には注意が必要です。特に変数AとCの影響を慎重に評価し、必要に応じてモデルの改善を検討することをお勧めします。\n"
     ]
    }
   ],
   "source": [
    "SYS_PROMPT = '''\n",
    "あなたは日本語AIアシスタントです。必ず日本語で回答します。\n",
    "'''\n",
    "\n",
    "r2 = 0.7\n",
    "vars = ['A', 'C', 'G']\n",
    "\n",
    "MESSAGE = '''\n",
    "予測モデルの決定係数が{}でした。以下の決定係数の目安に従って、精度を判定して下さい。\n",
    "また、予測モデルの変数重要度に{}が現れました。以下の注意が必要な変数に従って、警報を出す必要があるか判断して下さい。\n",
    "\n",
    "予測精度も低く、警報も出ている場合は注意が必要ですので、注意喚起を促して下さい。\n",
    "\n",
    "### 決定係数の目安\n",
    "0.3以下: 精度が低い\n",
    "0.3~0.7: 中位の精度\n",
    "0.7以上: 精度が高い\n",
    "\n",
    "### 注意が必要な変数\n",
    "変数AまたはBがあった場合: 警報X\n",
    "変数AとCがあった場合: 警報Y\n",
    "'''.format(r2, vars)\n",
    "\n",
    "MODEL = 'gpt-4-turbo-2024-04-09'\n",
    "\n",
    "with client.connect(chat_session_id) as session:\n",
    "    reply = session.query(\n",
    "        message = MESSAGE,\n",
    "        system_prompt=SYS_PROMPT,\n",
    "        llm=MODEL,\n",
    "        rag_config={\"rag_type\": \"llm_only\",},\n",
    "    )\n",
    "    print(reply.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "31c542ae-2a8a-417f-8648-0d74b8157885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "決定係数が0.2であるため、精度が低いと判断されます。\n",
      "\n",
      "変数重要度に['A', 'C', 'B']が現れているため、警報Yを出す必要があります。\n",
      "\n",
      "したがって、予測精度が低く、警報も出ているため、注意が必要です。注意喚起を促します。\n"
     ]
    }
   ],
   "source": [
    "SYS_PROMPT = '''\n",
    "あなたは日本語AIアシスタントです。必ず日本語で回答します。\n",
    "'''\n",
    "\n",
    "r2 = 0.2\n",
    "vars = ['A', 'C', 'B']\n",
    "\n",
    "MESSAGE = '''\n",
    "予測モデルの決定係数が{}でした。以下の決定係数の目安に従って、精度を判定して下さい。\n",
    "また、予測モデルの変数重要度に{}が現れました。以下の注意が必要な変数に従って、警報を出す必要があるか判断して下さい。\n",
    "\n",
    "予測精度も低く、警報も出ている場合は注意が必要ですので、注意喚起を促して下さい。\n",
    "\n",
    "### 決定係数の目安\n",
    "0.3以下: 精度が低い\n",
    "0.3~0.7: 中位の精度\n",
    "0.7以上: 精度が高い\n",
    "\n",
    "### 注意が必要な変数\n",
    "変数AまたはBがあった場合: 警報X\n",
    "変数AとCがあった場合: 警報Y\n",
    "'''.format(r2, vars)\n",
    "\n",
    "MODEL = 'microsoft/Phi-3-medium-128k-instruct'\n",
    "\n",
    "with client.connect(chat_session_id) as session:\n",
    "    reply = session.query(\n",
    "        message = MESSAGE,\n",
    "        system_prompt=SYS_PROMPT,\n",
    "        llm=MODEL,\n",
    "        rag_config={\"rag_type\": \"llm_only\",},\n",
    "    )\n",
    "    print(reply.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda7086e-a676-479a-ac65-2d549db0d862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
